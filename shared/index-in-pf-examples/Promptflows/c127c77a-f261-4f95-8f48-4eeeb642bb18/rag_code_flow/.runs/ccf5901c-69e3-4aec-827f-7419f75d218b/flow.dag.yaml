inputs:
  question:
    type: string
    is_chat_input: true
  chat_history:
    type: list
    is_chat_input: false
    is_chat_history: true
outputs:
  output:
    type: string
    reference: ${answer_the_question_with_context.output}
    evaluation_only: false
    is_chat_output: true
nodes:
- name: modify_query_with_history
  type: llm
  source:
    type: code
    path: modify_query_with_history.jinja2
  inputs:
    deployment_name: "gpt-35-turbo-16k-120k-tokens-default"
    temperature: 1
    top_p: 1
    max_tokens: 1200
    response_format:
      type: text
    presence_penalty: 0
    frequency_penalty: 0
    chat_history: "${inputs.chat_history}"
    question: "${flow.question}"
  api: chat
  provider: AzureOpenAI
  connection: Default_AzureOpenAI
  module: promptflow.tools.aoai
  aggregation: false
- name: embed_the_question
  type: python
  source:
    type: package
    tool: promptflow.tools.embedding.embedding
  inputs:
    connection: "Default_AzureOpenAI"
    deployment_name: "embedding-test-chat"
    input: "${modify_query_with_history.output}"
  aggregation: false
- name: search_question_from_indexed_docs
  type: python
  source:
    type: package
    tool: promptflow_vectordb.tool.vector_index_lookup.VectorIndexLookup.search
  inputs:
    path: "azureml://subscriptions/90a714cc-9046-4198-95fd-cddaf8e146d6/resourcegroups/rg-aidemoproject/providers/Microsoft.MachineLearningServices/workspaces/new-ai-demoproject/data/starfield/versions/1"
    query: "${embed_the_question.output}"
    top_k: 2
  aggregation: false
- name: generate_prompt_context
  type: python
  source:
    type: code
    path: generate_prompt_context.py
  inputs:
    search_result: "${search_question_from_indexed_docs.output}"
  aggregation: false
- name: answer_the_question_with_context
  type: llm
  source:
    type: code
    path: answer_the_question_with_context.jinja2
  inputs:
    deployment_name: "gpt-35-turbo-16k-120k-tokens-default"
    temperature: 0
    top_p: 1
    max_tokens: 1200
    presence_penalty: 0
    frequency_penalty: 0
    prompt_text: "${Prompt_variants.output}"
  api: chat
  provider: AzureOpenAI
  connection: Default_AzureOpenAI
  module: promptflow.tools.aoai
  aggregation: false
- name: Prompt_variants
  type: prompt
  source:
    type: code
    path: Prompt_variants__Variant_0.jinja2
  inputs:
    chat_history: "${flow.chat_history}"
    contexts: "${generate_prompt_context.output}"
    question: "${flow.question}"
  aggregation: false
environment:
  python_requirements_txt: requirements.txt
