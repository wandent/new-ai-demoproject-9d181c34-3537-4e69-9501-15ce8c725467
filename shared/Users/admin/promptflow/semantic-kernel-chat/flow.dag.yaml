id: template_chat_flow
name: Template Chat Flow
inputs:
  chat_history:
    type: list
    is_chat_input: false
    is_chat_history: true
  question:
    type: string
    is_chat_input: true
outputs:
  answer:
    type: string
    reference: ${sk_chat.output}
    is_chat_output: true
nodes:
- name: sk_chat
  type: python
  source:
    type: code
    path: sk_chat.py
  inputs:
    conn: Default_AzureOpenAI
    deployment_name: gpt-35-turbo-16k-120k-tokens-default
    history: ${chat_history.output}
    prompt: ${determine_intent.output}
    system: ${system.output}
  use_variants: false
- name: system
  type: prompt
  source:
    type: code
    path: system.jinja2
  inputs:
    text: ${document_context.output}
  use_variants: false
- name: chat_history
  type: prompt
  source:
    type: code
    path: chat_history.jinja2
  inputs:
    chat_history: ${inputs.chat_history}
  use_variants: false
- name: determine_intent
  use_variants: true
- name: document_context
  type: prompt
  source:
    type: code
    path: document_context.jinja2
  inputs: {}
  use_variants: false
node_variants:
  determine_intent:
    default_variant_id: variant_0
    variants:
      variant_0:
        node:
          name: determine_intent
          type: llm
          source:
            type: code
            path: determine_intent.jinja2
          inputs:
            deployment_name: gpt-35-turbo-16k-120k-tokens-default
            temperature: 1
            top_p: 1
            response_format:
              type: text
            presence_penalty: 0
            frequency_penalty: 0
            chat_history: ${inputs.chat_history}
            question: ${inputs.question}
          provider: AzureOpenAI
          connection: Default_AzureOpenAI
          api: chat
          module: promptflow.tools.aoai
      variant_1:
        node:
          name: determine_intent
          type: llm
          source:
            type: code
            path: determine_intent__variant_1.jinja2
          inputs:
            deployment_name: gpt-35-turbo-16k-120k-tokens-default
            temperature: 0.4
            top_p: 1
            response_format:
              type: text
            presence_penalty: 0
            frequency_penalty: 0
            chat_history: ${inputs.chat_history}
            question: ${inputs.question}
          provider: AzureOpenAI
          connection: Default_AzureOpenAI
          api: chat
          module: promptflow.tools.aoai
environment:
  python_requirements_txt: requirements.txt
